<!doctype html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>






<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="Notes for share and some plain essays">
<meta property="og:type" content="website">
<meta property="og:title" content="迷雾">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="迷雾">
<meta property="og:description" content="Notes for share and some plain essays">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="迷雾">
<meta name="twitter:description" content="Notes for share and some plain essays">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>迷雾</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  














  
  
    
  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">迷雾</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/19/Digital Communities Notes P3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiangfeng Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="迷雾">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/02/19/Digital Communities Notes P3/" itemprop="url">Digital Communities Notes P3</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-19T17:26:25+01:00">
                2017-02-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Recommender-Systems"><a href="#Recommender-Systems" class="headerlink" title="Recommender Systems"></a>Recommender Systems</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>A recommender system aims at <em>predicting</em> the rating or preference that a <em>user</em> would<br>give to an <em>item</em> that she has not yet considered.<br><strong><em>Goals</em></strong>: relevant, novel, serendipity, diverse.</p>
<h2 id="Collaborative-Filtering-Recommender-Systems"><a href="#Collaborative-Filtering-Recommender-Systems" class="headerlink" title="Collaborative Filtering Recommender Systems"></a>Collaborative Filtering Recommender Systems</h2><h3 id="Memory-based-methods"><a href="#Memory-based-methods" class="headerlink" title="Memory-based methods"></a>Memory-based methods</h3><p>Also called Neighborhood-based  CF.</p>
<h4 id="User-based-CF"><a href="#User-based-CF" class="headerlink" title="User-based CF"></a>User-based CF</h4><p><strong>Def</strong>: ratings provided by users that are similar to a target user <em>u</em> are used to make recommendations for <em>u</em>.</p>
<p>To determine the <strong>neighborhood </strong>of the target user, we need to compute <strong><em>similarity</em></strong> to all other users.</p>
<p>Target user <strong><em>u</em></strong>, the similarity of u and one of other users <strong><em>v</em></strong> is computed:</p>
<ul>
<li><p>Find the mutually observed ratings. (Given by both u and v)</p>
</li>
<li><p>Compute the mean rating, denoted as $\mu_u$ and $\mu_v$</p>
</li>
<li><p>Calculate Pearson correlation coefficient between u and v, $Covariance(u,v)/Standard Deviation(u) * Standard Deviation(v)$, the number of term depends on mutually observed ratings.</p>
<p>$\sum_{k\in mutually ratings}  (r_{uk}-\mu_u)*(r_{vk}-\mu_v)$</p>
</li>
<li>Let $P_u(j)$ be the set of users who are similar to the target user u and who have rated item j before.<br>predicted rating $\hat r_{uj}= \mu_u+\frac {\sum_{v\in P_u(j)}sim(u,v)(r_{vj}-\mu_v)}{\sum_{v\in P_u(j)}|sim(u,v)| }$</li>
</ul>
<p>$sim(u,v)$ is equal to $cos(u,v)$ where u is the rating vector of user u with all ratings subtract the mean value.</p>
<h5 id="Pros"><a href="#Pros" class="headerlink" title="Pros"></a>Pros</h5><ul>
<li>Works for any kind of item. No feature selection needed.</li>
</ul>
<h5 id="Cons"><a href="#Cons" class="headerlink" title="Cons"></a>Cons</h5><ul>
<li>Not all neighbor ratings might be equally “valuable”.<br>Agreement on commonly liked items is not so informative as agreement on controversial items. Tend to recommend popular items.<br>相似度高可能表现在都对 普遍好评 的产品给出高评价，这个不意味着两个用户之间真的相似。<ul>
<li>Give more weight to items that have a higher variance</li>
</ul>
</li>
<li>Value of number of co-rated items.<ul>
<li>Linearly reducing the weight when the number of co-rated items is low</li>
</ul>
</li>
<li>Case amplification<ul>
<li>Give more weight to “very similar” neighbors, i.e., where the similarity value is close to 1.</li>
</ul>
</li>
<li>Neighborhood selection<br>Use similarity threshold or fixed number of neighbors</li>
<li>High sparsity leads to few common ratings between two users.</li>
</ul>
<h4 id="Item-based-CF"><a href="#Item-based-CF" class="headerlink" title="Item-based CF"></a>Item-based CF</h4><p><strong>Def</strong>: firstly determine a set of other items <em>S</em>, which are most similar to item <em>i</em>. Then a weighted average of the ratings in <em>S</em> is used to predict the rating that <em>u</em> will likely give to <em>i</em>.<br>Item-to-item similarity is commonly based on measuring the <strong>cosine similarity</strong> between the rating vectors.</p>
<p>Let $U_i$ be the set of users who have rated item i before, and let $\mu_u$ be the mean rating of a user $u$.</p>
<p>the number of term depends on users rate both item $i$ and $j$.<br>$AdjustedCosine(i, j) = \frac{\sum_{u\in U_i\cap U_j}  (r_{uk}-\mu_u)<em>(r_{vk}-\mu_v)}{Standard Deviation(i)</em>Standard Deviation(j)} $<br>Let $Q_t(i)$ be the <strong>set of top-k items</strong> most similar to item $t$ for which user $u$ has specified ratings.<br>$\hat r_{ut}=\frac {\sum_{j\in Q_t(i)}AdjustedCosine(j,t) * r_{uj}}{\sum_{j\in Q_t(i)}|AdjustedCosine(j,t)| }$<br>找到相似度高的物品，利用该用户对其他物品的评价来评估对目标物品的评价。</p>
<h5 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h5><ul>
<li>We can calculate all pair-wise item similarities <em>in advance</em>, because item similarities are supposed to be more stable than user similarities.</li>
<li>The neighborhood to be used at run-time is typically rather small, because only items are taken into account which the user has rated.</li>
<li>Memory requirements<br>In theory, up to $n^2$ pair-wise similarities need to be memorized (n = number of items). In practice, the number may be smaller.</li>
<li>New items cannot be recommended.</li>
</ul>
<p>Further reductions possible, for example:</p>
<ul>
<li>Minimum threshold for co-ratings (items, which are rated at least by t users).</li>
<li>Limit the size of the neighborhood (might affect recommendation accuracy).</li>
</ul>
<p>A solution to the problem of rating sparsity is to segment users based on their profile, for example age, gender, country, education (a.k.a. demographic filtering).</p>
<h3 id="Model-based-Collaborative-Filtering"><a href="#Model-based-Collaborative-Filtering" class="headerlink" title="Model-based Collaborative Filtering"></a>Model-based Collaborative Filtering</h3><p>Model-based collaborative filtering relies on a summarized model of the data that is created beforehand during a training phase. This model is then used in the later prediction phase to efficiently calculate the predictions.</p>
<h4 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h4><ul>
<li>Space-efficiency: Typically, the size of the learned model is much smaller than the original ratings matrix.</li>
<li>Training speed and prediction speed: Model-based systems are usually much faster in the preprocessing phase of constructing the trained model.</li>
<li>Avoiding overfitting: The summarization approach of model-based methods can often help in avoiding overfitting.</li>
</ul>
<p>Model-based collaborative filtering methods include, for example, rule-based methods, decision trees, regression models, Bayes classifiers, support vector machines, or neural networks.</p>
<h4 id="Association-Rule-Mining"><a href="#Association-Rule-Mining" class="headerlink" title="Association Rule Mining"></a>Association Rule Mining</h4><p>Association Rule Mining is a technique to identify rulelike relationship patterns in large-scale transactions.</p>
<ul>
<li>An example rule could be: “If a customer purchases baby food then he or she also buys diapers in 70 percent of the cases”.  <code>[&#39;daɪəpə]</code>尿布</li>
</ul>
<p>The frequency of a given itemset is known as <em>support count</em>. The <em>support</em> of the itemset is the fraction of transactions that contain it. A frequent itemset is an itemset with a support that is greater or equal to a minsup threshold. </p>
<p>The <em>confidence</em> of the rule is how often items in Y appear in transactions that contain X.</p>
<p>A two-step approach: (1) Generate all itemsets with high <em>support</em> (Frequent Itemset Generation); (2) Generate high confidence rules from each frequent itemset (Rule Generation).</p>
<h4 id="Matrix-factorization-Singular-Value-Decomposition"><a href="#Matrix-factorization-Singular-Value-Decomposition" class="headerlink" title="Matrix factorization / Singular Value Decomposition"></a>Matrix factorization / Singular Value Decomposition</h4><p>Matrix factorization methods can be used in recommender systems to <em>derive a set of latent (hidden) factors</em> from the rating patterns and characterize both users and items by such vectors of factors.</p>
<p>$M = UΣV^T$</p>
<p>where U and V are called left and right singular vectors and the values of the diagonal of Σ are called the singular values.</p>
<p>The main point of the SVD is that we can approximate the full matrix by observing only the most important features, i.e., those with the largest singular values.</p>
<p>Matrix U and V correspond to the latent user and item factors. In M, user ratings are row vector, item ratings are column vector.</p>
<h2 id="Content-based-Recommender-Systems"><a href="#Content-based-Recommender-Systems" class="headerlink" title="Content-based Recommender Systems"></a>Content-based Recommender Systems</h2><p>Content-based recommender systems rely on a <strong>user’s own ratings</strong> and try to identify new items that are <strong>content-wise similar</strong> to the items the user has liked before.</p>
<p>Content-wise similarity is defined on the attributes of items.</p>
<p>Content-based recommender system relies on two concepts: user profile and item profile.</p>
<ul>
<li>User profile: previous explicit or implicit feedback the user has generated about various items.</li>
<li>Item profile: a set of content-centric attributes for a particular item, for example, a genre, a textual description, or a list of keywords. For example, a movie with a profile vector with each entry corresponding to one actor.</li>
</ul>
<h4 id="Feature-extraction-techniques"><a href="#Feature-extraction-techniques" class="headerlink" title="Feature extraction techniques"></a>Feature extraction techniques</h4><p>Let N be the number of all documents (items) D that can be recommended.<br>Let $f_{t,d}$ be the number of times that term t occurs in document d.<br>term frequency: $TF(t,d)=\frac {f_{t,d}}{max_x\lbrace f_{x,d}\rbrace}$</p>
<p>(divided by the maximum raw frequency of any term in the document)</p>
<p>inverse document frequency measures how common or rare a term is across all documents:</p>
<p>$IDF(t)=log\frac{N}{|\lbrace d\in D:t\in d\rbrace|}$</p>
<p>N divided by number of docs that mention term i.</p>
<p>term frequency-inverse document frequency(TF-IDF) is the multiply result.</p>
<p>It is large when the term occurs frequently in one document d, but occurs rarely across all documents.</p>
<p>TF-IDF is one possible measure to create individual item profiles:</p>
<p>$Profile(d)=(TFIDF(t_1,d),TFIDF(t_2,d),…,TFIDF(t_j,d))$ (Vector)</p>
<p>where j is the number of terms in d.</p>
<p>Similarity between two items can be calculated the cosine between the item profile vectors.</p>
<p>In practice, it is very common (and advisable) to preprocess documents before calculating TF-IDFs. Common steps are, for example:</p>
<ul>
<li>Removing stop words, i.e., very common terms such as “a”, “the”, “I”, “it”, etc.</li>
<li>Stemming, i.e., reducing all words to their stems.</li>
</ul>
<h4 id="Content-based-learning-of-user-profiles"><a href="#Content-based-learning-of-user-profiles" class="headerlink" title="Content-based learning of user-profiles"></a>Content-based learning of user-profiles</h4><p>Use profile of item rated by the user to calculate user-profile. </p>
<p>Weight sum of profiles of items according to ratings. </p>
<h4 id="Filtering-and-recommendation"><a href="#Filtering-and-recommendation" class="headerlink" title="Filtering and recommendation"></a>Filtering and recommendation</h4><p>Similarity between user-profile and possibly recommending item-profile.</p>
<h4 id="Pros-1"><a href="#Pros-1" class="headerlink" title="Pros"></a>Pros</h4><ul>
<li>No need for data on other users.</li>
<li>Able to recommend new items. No first-rater problem.</li>
<li>Can explain why items are recommended (add features selected)</li>
</ul>
<h4 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h4><ul>
<li>Data analysis: all items must be available in a data format that can be automatically parsed in order to extract the features. (hard for audio or video streams )</li>
<li>Equality of features: If two items are represented by the same set of features, it is not possible anymore to distinguish between the two.</li>
<li>Overspecialization: can occur when only items with a very high degree of utility are recommended. Then, if a user has never rated items from a certain group of items, he will not get any recommendations for them. (introduce some randomness in order to increase the diversity of recommendations)</li>
<li>New user problem: inaccurate due to few ratings. How to build a user profile?</li>
</ul>
<h2 id="Knowledge-based-Recommender-Systems"><a href="#Knowledge-based-Recommender-Systems" class="headerlink" title="Knowledge-based Recommender Systems"></a>Knowledge-based Recommender Systems</h2><p>Knowledge-based recommender systems rely on explicitly soliciting user requirements for items. Well suited to the recommendation of items that are not bought on a regular basis (for example, house, car, etc.).  <code>[sə&#39;lɪsɪt]</code></p>
<p>Constraint-based Recommender Systems: users specify requirements or constraints (for example, lower or upper limits) on the item.</p>
<p>Three types of input for a constrained-based recommender system:</p>
<ul>
<li>Inherent properties of the user (e.g, demographics, risk profiles) and specific requirements in the product.</li>
<li>Knowledge bases, which map user attributes to products attributes. For example, a user who is searching for a house and who has a family size of 5 will likely require ≥ 3 bedrooms.</li>
<li>The product catalog, which contains all the item attributes.</li>
</ul>
<p>Case-based Recommender Systems: specific cases are specified by the user as targets or anchor points.</p>
<p>A similarity function is used to retrieve the examples that are most <em>similar</em> to the <em>user-specified target</em>. </p>
<h2 id="Evaluating-Recommender-Systems"><a href="#Evaluating-Recommender-Systems" class="headerlink" title="Evaluating Recommender Systems"></a>Evaluating Recommender Systems</h2>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/18/Digital Communities Notes P2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiangfeng Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="迷雾">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/02/18/Digital Communities Notes P2/" itemprop="url">Digital Communities Notes P2</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-18T17:45:56+01:00">
                2017-02-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Network-Analysis"><a href="#Network-Analysis" class="headerlink" title="Network Analysis"></a>Network Analysis</h1><h2 id="Network-density"><a href="#Network-density" class="headerlink" title="Network density"></a>Network density</h2><p>a simple, undirected graph $G$ with $n$ vertices and $m$ edges.</p>
<p><strong>Mean degree</strong> $c=\frac{1}{n}\sum_{i=1}^n\delta(i)=\frac{2m}{n}$</p>
<p><strong>Network density</strong> $ρ(G)=\frac{m}{n \choose 2}=\frac{2m}{n(n-1)}=\frac{c}{n-1}$</p>
<p>If the density ρ tends to a <em>constant</em> as $n → ∞$, the network is said to be <strong>dense</strong>.</p>
<p>A network in which $ρ → 0$ as $n → ∞$ is said to be <strong>sparse</strong>.</p>
<h2 id="Average-Path-Length"><a href="#Average-Path-Length" class="headerlink" title="Average Path Length"></a>Average Path Length</h2><p>$\bar d(u)=\frac{1}{|V|-1}\sum_{v\in V,v\neq u}d(u,v)$ 点</p>
<p>$\bar d(G)=\frac{1}{|V|}\sum_{u\in V}\bar d(u)$ 图</p>
<p>The <strong>characteristic path length</strong> of G is defined as the <strong>median</strong> over all $\bar d(u)$. 中位数</p>
<h2 id="Clustering-Coefficient"><a href="#Clustering-Coefficient" class="headerlink" title="Clustering Coefficient"></a>Clustering Coefficient</h2><p>a simple connected, undirected graph G</p>
<p>vertex $v ∈ V(G)$ with neighbor set $N(v)$. Let $n_v = |N(v)|$, $m_v$ be the number of edges in the <em>subgraph induced</em> by $N(v)$, $m_v = |E(G[N(v)])|$.</p>
<p>$cc(v)= \begin{cases} \frac{m_v}{n_v \choose 2}=ρ(G[N(v)]) &amp; \text{if $\delta(v)&gt;1$} \\undefined &amp; \text{otherwise} \end{cases}$</p>
<p>$cc(G)$对所有$\delta(v)&gt;1$的点的cc取平均。</p>
<h2 id="Degree-Correlation"><a href="#Degree-Correlation" class="headerlink" title="Degree Correlation"></a>Degree Correlation</h2><p>Let G be a simple graph with degree sequence $d = [d1, . . . , dn]$.</p>
<p>$r_{deg}(G)=\frac{Covariance}{Variance}=\frac{\sum^n_{i=1}\sum^n_{j=i+1}((d_i-\bar d)(d_j-\bar d)A_{ij})}{\sum^n_{i=1}(d_i-\bar d)^2}$</p>
<p>Note: $j=i+1$, 取矩阵对角线右上方，$A_{ij}=1$的$i,j$计算。</p>
<p>意义不明。比如regular图，所有点的度数相同，分母为0。</p>
<h2 id="Assortative-Mixing"><a href="#Assortative-Mixing" class="headerlink" title="Assortative Mixing"></a>Assortative Mixing</h2><p>The network is called <strong>assortative</strong> if a significant fraction of the edges in the network run between nodes of the same type.</p>
<p>People have, it appears, a tendency to associate with others whom they perceive as being <em>similar</em>. Such a tendency is also called <strong>homophily </strong>or <strong>assortative mixing</strong>.</p>
<h3 id="Assortative-mixing-by-enumerative-characteristics"><a href="#Assortative-mixing-by-enumerative-characteristics" class="headerlink" title="Assortative mixing by enumerative characteristics"></a>Assortative mixing by enumerative characteristics</h3><p>enumerative <code>[ɪ&#39;njʊməretɪv]</code></p>
<p>a finite set of classes (labels) for the nodes. For example, genre, language, school class etc.</p>
<p><strong>Goal</strong>: the network is assortative if a significant fraction of the edges run between vertices of the same type.</p>
<p>We want a measure that is <em>large</em> for <em>non-trivial</em> cases and <em>small</em> for <em>trivial</em> ones.  <code>[&#39;trɪvɪəl]</code></p>
<p><strong>Idea</strong>: we calculate the fraction of edges that run between vertices of the same type, and then we <em>subtract</em> from that figure the fraction of such edges we find if edges were positioned at <em>random</em> without regards to vertex type.</p>
<p>计算相同种类之间的边的数目 - 随机分布情况下相同种类之间的边的数目</p>
<p>$c_i$ : the class or type of vertex <em>i</em>.</p>
<p>$n_c$: number of classes.</p>
<p>The total number of edges that are incident to vertices of the same type:</p>
<p>$N_{same}=\sum_{edges(i,j)} \delta(c_i,c_j) = \frac{1}{2}\sum_{ij}A_{ij}*\delta(c_i,c_j)$</p>
<p>where $\delta(x,y)= \begin{cases} 1 &amp; x=y\\0 &amp;x\neq y \end{cases}$</p>
<p>The expected number of edges between all pairs of vertices of the same type:</p>
<p>$N_{rand}=\frac{1}{2}\sum_{ij}\frac{k_ik_j}{2m}\delta(c_i,c_j)$</p>
<p>$k_i$ is the degree of vertex $i$. </p>
<p>$P(A)$ =P(particular edge is one of the $k_j$ ends attached to vertex $j$) = $k_j/2m$</p>
<p>$P(AB)$ = P(particular edge between one of $k_i$ and one of $k_j$ ends) = $k_i/2m * k_j/2m$</p>
<p>Number of the edges = E(particular edge between i and j) = $m<em>P(AB)=\frac{1}{2}</em>\frac{k_ik_j}{2m}$</p>
<p><strong>modularity</strong>: measure the strength of division of a network into modules (also called groups, clusters or communities). Networks with <em>high</em> modularity have <em>dense</em> connections between the nodes <em>within modules</em> but <em>sparse</em> connections between nodes <em>in different modules</em>.</p>
<p>$Q = \frac {N_{same}-N_{rand}}{m} = \frac{1}{2m}\sum_{ij}(A_{ij}-\frac{k_ik_j}{2m})\delta(c_i,c_j)$</p>
<h3 id="Assortative-mixing-by-scalar-characteristics"><a href="#Assortative-mixing-by-scalar-characteristics" class="headerlink" title="Assortative mixing by scalar characteristics"></a>Assortative mixing by scalar characteristics</h3><p>scalar characteristics such as age or income, i.e., values that are ordered.</p>
<p>A common approach to determine assortative mixing by scalar characteristics is to use a <strong>covariance measure</strong>.</p>
<p>Let $x_i$ be the value for vertex i of the scalar value. (每个点对应一个表示类型的数值)</p>
<p>$均值=\sum_边\frac{每条边的两个端点的x值}{所有边的端点数量=2m}$</p>
<p><strong>Mean</strong> $\mu$ of the value of $x_i$ at the <em>end of an edge</em>:  $\mu=\frac{\sum_{ij}A_{ij}x_i}{2m} = \frac{\sum_i k_ix_i}{2m}$</p>
<p><strong>covariance</strong> of $x_i$ and $x_j$ over edges is:</p>
<p>$cov(x_i,x_j)=\frac{\sum_{ij}A_{ij}(x_i-\mu)(x_j-\mu)}{2m}=(\frac 1 {2m}\sum_{ij}A_{ij}x_ix_j)-\mu^2$</p>
<p>$\mu^2=\frac 1 {(2m)^2}\sum_{ij}k_ik_jx_ix_j$</p>
<p>$cov(x_i,x_j)=\frac 1 {2m} \sum_{ij}(A_{ij}-\frac {k_ik_j}{2m})x_ix_j$</p>
<p>The covariance will be <strong>positive</strong>, if, overall, $x_i$ and $x_j$ at an incident edge are <em>both large or both small</em> and negative if they tend to vary in opposite directions.</p>
<p>To <strong>normalize</strong> is to make it 1 in a <em>perfectly mixed network</em> (all edges fall between vertices with precisely equal values of $x_i$). 意味着$x_i=x_j$当$A_{ij}=1$.</p>
<p>So the covariance value for <em>perfectly mixed network</em><br>$V_{perfect}=\frac 1 {2m} \sum_{ij}(A_{ij}x_i^2-\frac {k_ik_j}{2m}x_ix_j)=\frac 1 {2m} \sum_{ij}(k_i\delta_{ij}-\frac {k_ik_j}{2m})x_ix_j$</p>
<p>这里第二个等号有问题啊….$\delta{ij}$解释不清，求和号意味着i和j不会相等。</p>
<p><strong>assortativity coefficient</strong>:</p>
<p>$r =cov(x_i,x_j)/V_{perfect}$</p>
<h2 id="Centrality"><a href="#Centrality" class="headerlink" title="Centrality"></a>Centrality</h2><h3 id="Degree-centrality"><a href="#Degree-centrality" class="headerlink" title="Degree centrality"></a>Degree centrality</h3><p>the degree of a vertex.</p>
<h3 id="Eigenvector-centrality"><a href="#Eigenvector-centrality" class="headerlink" title="Eigenvector centrality"></a>Eigenvector centrality</h3><p><strong>Idea</strong>: extend the concept of degree centrality by <em>increasing a vertex’s importance</em> if it is connected to other vertices that are themselves important.</p>
<p><strong>Goal</strong>: give each node a <em>score</em> <strong>proportional to</strong> the <em>sum of the scores</em> of its neighbors.</p>
<p>对于undirected graph, 实对称矩阵A的不同特征值所对应的特征向量是正交的实向量。实对称矩阵A的特征值都是实数。</p>
<p>$\hat x_i = \sum_j A_{ij}x_j$</p>
<p>After t steps, we have a vector of centralities x(t) given by $x(t)=A^tx(0)$</p>
<p>Write x(0) as a linear combination of eigenvectors $v_i$ of A:</p>
<p>$x(0) =\sum_i c_iv_i$</p>
<p>$x(t)=A^t\sum_i c_iv_i=\sum_i c_ik_i^tv_i=k_1^t\sum_i c_i[\frac {k_i}{k_1}]^t$</p>
<p>$k_i$ are eigenvalues of A and $k_1$is the largest. When $t → ∞$, $x(t)→c_1k_1^tv_1$</p>
<p>x(t) can be considered as $c_1v_1$, $x(t)$ is a eigenvector of A.</p>
<p>$Ax=k_1x$</p>
<p>The eigenvector centrality has the nice property that it can be large  ither because a vertex has many neighbors or because it has important neighbors (or both).</p>
<p>All eigenvector centralities of all vertices are non-negative.</p>
<p>Trap: if a vertex with no out-link, it will absorb centrality. Or all out-links are within the group. In this situation, it will make the eigenvector centrality very large.</p>
<h4 id="Cons-in-directed-network"><a href="#Cons-in-directed-network" class="headerlink" title="Cons in directed network:"></a>Cons in directed network:</h4><ul>
<li>A is asymmetric, this results in two leading eigenvectors.<br>In most cases it is better to use the <em>right eigenvector</em>, given that centrality in directed networks is bestowed by other vertices pointing towards you.</li>
<li>A vertex with no incoming edges will always have a centrality of zero. This also affects its connected vertices, their centrality will also be zero.</li>
</ul>
<h3 id="Katz-Centrality"><a href="#Katz-Centrality" class="headerlink" title="Katz Centrality"></a>Katz Centrality</h3><p>Give each vertex a small amount $\beta$ of centrality for free:</p>
<p>$x_i=\alpha \sum_j A_{ij}x_j+\beta$</p>
<p>$x=\alpha Ax+\beta 1$</p>
<p>$x=(I-\alpha A)^{-1}\beta 1$</p>
<p>As we increase $\alpha$ from zero the centrality increases and eventually teher comes a point where they diverge. This happens where $(I-\alpha A)^{-1}$ diverges, i.e., when $det(I-\alpha A)$ passes through zero:</p>
<p>$det(A-\alpha ^{-1}I=0)$ </p>
<p>The determinant first crosses zero when $\alpha ^{-1}$ equals the largest eigenvalue of A. We should choose a value of $\alpha$ less than $1/k_1$ because then we can be sure that the corresponding matrix is invertible.</p>
<p>Most researchers employ values of $\alpha$ close to $1/k_1$.</p>
<p>Neumann series to invert a matrix M: $M^{-1}= \sum_{k=0}^\infty (I-M)^k$ and stop after an appropriate number of iterations.</p>
<h3 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h3><p>Suppose web pages as vertices, super link as directed links.</p>
<p>Out-going link is easier than in-coming link. So in-coming link from important pages should count more. But a high-centrality vertex pointing to many others gives the others also high centrality.</p>
<p>So we split all of it PageRank to out links evenly.</p>
<p>Flow of PageRank could be considered as flow of current in one node.</p>
<p>$x_i=\alpha \sum_j A\frac {x_j} {k_j^{out}}+\beta$</p>
<p>We require $k_j^{out}$ to be set to 1 in case the vertex has no outgoing edges.</p>
<p>$x=\alpha AD^{-1}x+\beta 1$</p>
<p>with D being the diagnoal matrix with elements $D_{ii}=max(k_i^{out},1)$ and we set $\beta =1$.</p>
<p>$x=(I-\alpha AD^{-1})^{-1}1=[(D-\alpha A)D^{-1}]^{-1}1=D(D-\alpha A)^{-1}1$</p>
<p>$\alpha$ should be less than the inverse of the largest eigenvalue of $AD^{-1}$.</p>
<p>For an undirected network, this largest eigenvalue turns out to be 1.</p>
<p>For a directed network in practical cases the leading eigenvalue will be roughly of order 1.</p>
<p>For example, a common value is α = 0.85.</p>
<h3 id="Closeness-centrality"><a href="#Closeness-centrality" class="headerlink" title="Closeness centrality"></a>Closeness centrality</h3><p>$C_c(i)=\frac{n-1}{\sum_j d(i,j)}$</p>
<p>inverse of mean distance.</p>
<h3 id="Betweenness-Centrality"><a href="#Betweenness-Centrality" class="headerlink" title="Betweenness Centrality"></a>Betweenness Centrality</h3><p>measures the extent to which a vertex lies on paths between other vertices.</p>
<p>$C_B(v) = \sum_{s\neq v\neq t \in V} \frac{\sigma _{st}(v)}{\sigma _{st}}$</p>
<p>$σ_{st}$ denote the number of shortest paths from $s ∈ V$ to $t ∈ V$.</p>
<p>$σ_{st}(v)$ denote the number of shortest paths from s to t that some $v ∈ V$ lies on. </p>
<p>For undirected networks the <em>normalized</em> betweenness centrality is given by  $C_B(v) /C_{n-1}^2$.</p>
<p>For directed networks the <em>normalized</em> betweenness centrality is given by $C_B(v) /A_{n-1}^2$.</p>
<h1 id="Random-networks"><a href="#Random-networks" class="headerlink" title="Random networks"></a>Random networks</h1><p>A random graph is a model network in which some specific set of parameters take fixed values, but the network is random in other respects.</p>
<h2 id="Erdos-Renyi-Model"><a href="#Erdos-Renyi-Model" class="headerlink" title="Erdös Rényi Model"></a>Erdös Rényi Model</h2><p>also ER random graph $G(n,p)$ is an undirected graph with n nodes in which each two distinct vertices are connected by an edge with probability p.</p>
<p>The set of all ER random graphs, with n vertices and probability p that two vertices are joined, is denoted by ER(n, p).</p>
<p>The clustering coefficient of any ER(n, p) is equal to p.</p>
<h2 id="Watts-Strogatz-Random-Graph"><a href="#Watts-Strogatz-Random-Graph" class="headerlink" title="Watts-Strogatz Random Graph"></a>Watts-Strogatz Random Graph</h2><p>WS(n, k, p) $n\gg k \gg ln(n)\gg 1  $.</p>
<ol>
<li><p>Order the n vertices into a ring and connect each vertex to its first k/2 left-hand (clockwise) neighbors, and its k/2 right-hand (counterclockwise) neighbors, leading to graph G.</p>
</li>
<li><p>With probability p, replace each edge $&lt; u, v &gt;$ with an edge $&lt; u, w &gt;$ where w is a<br>randomly chosen vertex from V(G) other than u, and such that $&lt; u, w &gt;$ is not already<br>contained in the edge set of (the modified) G.</p>
</li>
</ol>
<p>$\bar d(u) \approx \frac{(n-1)(n+k-1)}{2kn} $</p>
<p>WS(n, k, 0) may show a high clustering coefficient, yet it has large average shortest path lengths.</p>
<p>However, if we slightly increase p, the average path length and cc drops rapidly.</p>
<h2 id="Small-World-Networks"><a href="#Small-World-Networks" class="headerlink" title="Small World Networks"></a>Small World Networks</h2><p>In many real-world situations, the average shortest path length is relatively small.</p>
<p>Small world networks show properties of ER random graphs, however they differ in terms of their clustering coefficient and average shortest path length.</p>
<p>Watts-Strogatz random graphs are generally considered to represent small-world phenomenon. However, WS random graphs often do not capture (other) properties of real-world networks, such as communication networks or biological networks.</p>
<h2 id="Scale-free-Networks"><a href="#Scale-free-Networks" class="headerlink" title="Scale-free Networks"></a>Scale-free Networks</h2><p>A network is called scale-free if the distribution of vertex degrees follows a power law, which means that the probability that an arbitrary node has degree k is proportional to $(1/k)^α$ for some number α &gt; 1 (called the scaling exponent).</p>
<p>Typical values for α are in the range $2 ≤ α ≤ 3$.</p>
<h1 id="Social-Network-Analysis"><a href="#Social-Network-Analysis" class="headerlink" title="Social Network Analysis"></a>Social Network Analysis</h1><p>When analyzing the relationships between pairs (dyads) of social entities, we can derive secondary relationships.</p>
<p>dyad <code>[&#39;daɪæd]</code> n.  一对</p>
<p>dyadic <code>[daɪ&#39;ædɪk]</code> a.</p>
<h2 id="Cocitation"><a href="#Cocitation" class="headerlink" title="Cocitation"></a>Cocitation</h2><p>The cocitation $C_{ij}$ of two vertices i and j in a directed network is the number of vertices that have outgoing edges pointing to both i and j:</p>
<p>$C_{ij}=\sum_{k=1}^n A_{ik}A_{jk}= \sum_{k=1}^n A_{ik}A^T_{kj}$</p>
<p>后面是矩阵乘法定义。</p>
<p>cocitation matrix $C=AA^T$, C is symmetric.</p>
<h2 id="Bibliographic-Coupling"><a href="#Bibliographic-Coupling" class="headerlink" title="Bibliographic Coupling"></a>Bibliographic Coupling</h2><p><code>[,bɪblɪə&#39;ɡræfɪk]</code></p>
<p>The bibliographic coupling $B_{ij}$ of two vertices i and j in a directed network is the number of other vertices to which both point:</p>
<p>$B_{ij}=\sum_{k=1}^n A_{ki}A_{kj}= \sum_{k=1}^n A^T_{ik}A_{kj}$</p>
<p>bibliographic coupling matrix $B=A^TA$.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/02/17/Digital Communities Notes P1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jiangfeng Du">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="迷雾">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/02/17/Digital Communities Notes P1/" itemprop="url">Digital Communities Notes P1</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-02-17T17:24:32+01:00">
                2017-02-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Foundations-of-Graph-Theory"><a href="#Foundations-of-Graph-Theory" class="headerlink" title="Foundations of Graph Theory"></a>Foundations of Graph Theory</h1><h2 id="Graphs-Vertices-and-Edges"><a href="#Graphs-Vertices-and-Edges" class="headerlink" title="Graphs, Vertices and Edges"></a>Graphs, Vertices and Edges</h2><p>Graph $G = (V, E)$, Set of vertices $V(G)$, Set of edges $E(G)$.</p>
<blockquote>
<p>Each edge $e\in E$ is said to <strong>join</strong> two vertices, which are called its <strong>end points</strong>.</p>
<p>I If $e$ joins $u, v∈V$, we write $e = <u, v="">$. Vertex $u$ and $v$ in this case are said to be <strong>adjacent</strong>. Edge e is said to be <strong>incident </strong>with vertices $u$ and $v$, respectively.</u,></p>
</blockquote>
<h2 id="Loops-and-Multiple-Edges"><a href="#Loops-and-Multiple-Edges" class="headerlink" title="Loops and Multiple Edges"></a>Loops and Multiple Edges</h2><p>Loop (self-edge)  </p>
<p>Multiple edges (multi-edge)</p>
<blockquote>
<p>A graph that does not have loops or multiple edges is called <strong>simple</strong>.</p>
</blockquote>
<h2 id="Complete-Graphs"><a href="#Complete-Graphs" class="headerlink" title="Complete Graphs"></a>Complete Graphs</h2><blockquote>
<p>A simple graph having n vertices, with each vertex being adjacent to every other vertex is called a <strong>complete graph</strong> $K_n$.</p>
</blockquote>
<h2 id="Degree"><a href="#Degree" class="headerlink" title="Degree"></a>Degree</h2><blockquote>
<p>The number of edges incident with a vertex $v ∈ V$ is called the <strong>degree </strong>of $v$, denoted as $δ(v)$.</p>
<p>The <strong>maximal degree </strong>of a vertex in graph $G$ is denoted as $\Delta(G)$.</p>
</blockquote>
<p>For any graph, the number of vertices with odd degree is even.</p>
<h2 id="Adjacency-matrix-of-a-Graph"><a href="#Adjacency-matrix-of-a-Graph" class="headerlink" title="Adjacency matrix of a Graph"></a>Adjacency matrix of a Graph</h2><p>$$A_{ij} = \begin{cases} 1, &amp; \text{an edge between vertices $i$ and $j$} \ 0, &amp; \text{otherwise} \end{cases}$$</p>
<h2 id="Directed-Graphs"><a href="#Directed-Graphs" class="headerlink" title="Directed Graphs"></a>Directed Graphs</h2><p>Digraph $D = (V , A)$, set of vertices $V$ and set of arcs $A$.</p>
<p>arc $a = <u,v>$ is said to join vertex $u ∈ V$ to another vertex $v\in V$.</u,v></p>
<p>Vertex $u$ is called the <strong>tail</strong> of $a$, whereas $v$ is its <strong>head</strong>.</p>
<p>Number of arcs with head <em>v</em> : in-degree $δ_{in}(v)$.</p>
<p>Number of arcs with tail <em>v</em> : out-degree $δ_{out}(v)$.</p>
<p>$$A_{ij} = \begin{cases} 1, &amp; \text{an arc that joins $j$ to $i$} \ 0, &amp; \text{otherwise} \end{cases}$$</p>
<h2 id="Walks-Trails-Paths-and-Cycles"><a href="#Walks-Trails-Paths-and-Cycles" class="headerlink" title="Walks, Trails, Paths, and Cycles"></a>Walks, Trails, Paths, and Cycles</h2><p>A $(v0, vk)-walk$ in G is an alternating sequence $[v0, e1, v1, e2, . . . vk−1, ek, vk]$ of vertices and edges from G with $e_i = <v_{i−1}, v_i="">.$</v_{i−1},></p>
<p>A <strong>trail </strong>is a <em>walk</em> in which all <em>edges are distinct</em>.</p>
<p>A <strong>path </strong>is a <em>trail</em> in which all <em>vertices are distinct</em>.</p>
<p>A <strong>cycle </strong>is a closed trail in which all vertices excepts $v_0$ and $v_k$ are distinct.</p>
<p>A simple, connected graph having no cycles is called a <strong>tree </strong>(also called <strong>acyclic</strong><br><strong>graph</strong>).</p>
<h2 id="Connectedness-and-Components"><a href="#Connectedness-and-Components" class="headerlink" title="Connectedness and Components"></a>Connectedness and Components</h2><p>A graph $H$ is a <strong>subgraph</strong> of $G$ if $V(H) ⊆ V(G)$ and $E(H) ⊆ E(G)$ such that for all<br>$e ∈ E(H)$ with $e = &lt; u, v &gt;$ we have that $u, v ∈ V(H)$.</p>
<p>A subgraph $H$ of $G$ is called a <strong>component</strong> of $G$ if $H$ is <em>connected</em> and <em>not contained</em> in a connected subgraph of $G$ with more vertices or edges.</p>
<p>The number of components of G is denoted as $ω(G)$.</p>
<p>The adjacency matrix of a network with more than one component can be written in <em>block diagonal form</em>.</p>
<h2 id="Vertex-Eccentricity-Graph-Radius-and-Diameter"><a href="#Vertex-Eccentricity-Graph-Radius-and-Diameter" class="headerlink" title="Vertex Eccentricity, Graph Radius and Diameter"></a>Vertex Eccentricity, Graph Radius and Diameter</h2><p>Eccentricity <code>[,eksen&#39;trɪsɪtɪ]</code></p>
<p>The <strong>(geodesic) distance</strong> between $u$ and $v$, denoted as $d(u, v)$, is the length of a<br>shortest $(u, v)-path$.</p>
<p>The <strong>eccentricity</strong> $\epsilon (u)$ of a vertex $u$ in $G$ is defined as $max\{d(u, v)|v ∈ V(G)\}$.</p>
<p>The <strong>radius</strong> of $G$, denoted as $rad(G)$, is equal to $min\{\epsilon (u)|u ∈ V(G)\}$.</p>
<p>The <strong>diameter</strong> of $G$, denoted as $diam(G)$, is the maximal shortest path between any two vertices: $diam(G) := max\{d(u, v)|u, v ∈ V(G)\}$.</p>
<p>The eccentricity of a vertex $u$ tells us <em>how far the farthest vertex from $u$</em> is positioned in the network.</p>
<p>The radius of a network is an indication of how <em>disparate</em> the vertices in a network actually are.</p>
<p>disparate <code>[&#39;dɪsp(ə)rət]</code> <code>a.</code> clearly different from each other in quality or type.</p>
<h2 id="Clique-Clan-and-Core"><a href="#Clique-Clan-and-Core" class="headerlink" title="Clique, Clan and Core"></a>Clique, Clan and Core</h2><p>undirected simple graph G. </p>
<p>A <strong>(maximal) clique</strong> of $G$ is a <em>maximal complete</em> subgraph $H$ of at least three vertices. A clique with $k$ vertices is called a $k-clique$.</p>
<p>A <strong>k-distance-clique</strong> of $G$ is a <em>maximal</em> subgraph H of G such that for all vertices $u, v ∈ V(H)$, the distance $d_G(u, v) ≤ k$.</p>
<p>A <strong>k-clan</strong> of $G$ is a <em>k-distance-clique H</em> of G such that for all vertices $u, v∈V(H)$, the distance $d_H(u, v) ≤ k$.</p>
<p>A <strong>k-core</strong> of G is a <em>maximal</em> subgraph H of G such that for all vertices $u∈V(H)$, the degree $δ(u) ≥ k$.</p>
<h2 id="Regular-Graphs"><a href="#Regular-Graphs" class="headerlink" title="Regular Graphs"></a>Regular Graphs</h2><blockquote>
<p>If every vertex has the same degree, the graph is called <strong>regular</strong>.</p>
</blockquote>
<h2 id="Vertex-Cuts-and-Edge-Cuts"><a href="#Vertex-Cuts-and-Edge-Cuts" class="headerlink" title="Vertex Cuts and Edge Cuts"></a>Vertex Cuts and Edge Cuts</h2><p>割点集合和割边集合</p>
<p>The size of a <em>minimal vertex cut</em> for graph G is denoted as $\kappa(G)$.</p>
<p>The size of a <em>minimal edge cut</em> for graph G is denoted as$ λ(G)$.</p>
<p>$κ(G) ≤ λ(G) ≤ min\{δ(v)|v ∈ V(G)\}$</p>
<p>A graph G is k-connected if $κ(G) ≥ k$.</p>
<p>G is k-edge-connected if $λ(G) ≥ k$. </p>
<p>G is optimally connected if $κ(G) = λ(G) = min\{δ(v)|v ∈ V(G)\}$.</p>
<h2 id="Graph-Isomorphism"><a href="#Graph-Isomorphism" class="headerlink" title="Graph Isomorphism"></a>Graph Isomorphism</h2><p>isomorphism <code>[,aɪsəʊ&#39;mɔːfɪzəm]</code> 同形性, 同态性</p>
<p>$G$ and $G^∗$ are <strong>isomorphic</strong> if there exists a one-to-one mapping $φ : V → V^∗$ such that for every edge $e ∈ E$ with $e = <u, v="">$, there is a <em>unique</em> edge $e^∗ ∈ E^∗$ with $e^∗ = &lt;φ(u), φ(v)&gt;.$</u,></p>
<p>$φ$是点的映射，边是一对一对应。</p>
<p>Necessary conditions: same degree sequence</p>
<p>In general, graph isomorphism plays an important role in complexity theory. It is in NP, but it is not known to be in P and it is not known to be NP-complete.</p>
<p>Put simply, for a given graph $G$ it is computationally <strong>easy</strong> to create an isomorphic graph $G^∗$. However, for two given graphs $G$ and $G^<em>$ it is computationally <em>*hard</em></em> to reconstruct an isomorphic mapping function $φ$.</p>
<h1 id="Graph-Embedding"><a href="#Graph-Embedding" class="headerlink" title="Graph Embedding"></a>Graph Embedding</h1><p>visualization of a graph</p>
<blockquote>
<p>A <strong>graph embedding</strong> is a representation of a graph on a surface where every vertex is mapped to a particular position on the surface.</p>
</blockquote>
<h2 id="Circular-Embedding"><a href="#Circular-Embedding" class="headerlink" title="Circular Embedding"></a>Circular Embedding</h2><p>Place vertices at evenly spaced points on a surface.</p>
<p>A circular embedding is particularly <em>useful</em> if one wants to <em>see all edges</em>.</p>
<p>Because no three vertices ever lie on the same line, this allows every edge to stay (reasonably) visible.</p>
<h2 id="Spring-Embedding"><a href="#Spring-Embedding" class="headerlink" title="Spring Embedding"></a>Spring Embedding</h2><p>The spring embedding was proposed by Eades in 1984. </p>
<p>Attracting force $F_{att}(u, v)$ between the vertices u and v it joins as follows:</p>
<p>$$F_{att}(u, v)\stackrel{def}=  \begin{cases} 2log(d(u,v)) &amp; \text{if u and v are adjacent} \\0 &amp; \text{otherwise} \end{cases}$$</p>
<p>吸引力F随着距离增加按log增长。</p>
<p>Repelling force $F_{rep}(u, v)$ is defined as:</p>
<p>$$F_{rep}(u, v)\stackrel{def}=  \begin{cases} 0 &amp; \text{if u and v are adjacent} \\1/\sqrt{(d(u,v))} &amp; \text{otherwise} \end{cases}$$</p>
<p>随着距离增加，力按根号倒数速度衰减。</p>
<p>Algorithm:</p>
<ol>
<li><p>Place all vertices randomly.</p>
</li>
<li><p>For each vertex $u$ calculate the current forces in the $x$ and $y$ direction, respectively:</p>
<p>$F_x(u)\stackrel{def}=\sum_{v\neq u}(F_{att,x}(u,v)-F_{rep,x}(u,v)) $</p>
<p>$F_y(u)\stackrel{def}=\sum_{v\neq u} (F_{att,y}(u,v)-F_{rep,y}(u,v)) $</p>
</li>
<li><p>Reposition vertex u according to:<br>$u_x = u_x + 0.1<em>F_x(u)$  and  $u_y = u_y + 0.1</em>F_y(u)$</p>
</li>
<li><p>Goto step 2. Stop after M iterations.</p>
<p>Here, $F_x(u,v)=F(u,v)* \frac{|v_x-u_x|}{d(u,v)} $</p>
</li>
</ol>
<p>每个点，有来自数量等于度的相邻结点的吸引力，和与其他所有不相邻的点的排斥力。力是根据二维平面距离计算的，但是计算坐标时是投影到x轴和y轴进行计算的。</p>
<h2 id="Planar-Graphs"><a href="#Planar-Graphs" class="headerlink" title="Planar Graphs"></a>Planar Graphs</h2><p><code>[&#39;pleɪnə]</code></p>
<blockquote>
<p>A <strong>plane graph</strong> is a specific embedding of a graph G such that no two edges <em>intersect</em>. If such an embedding exists, G is said to be <em>planar</em>.</p>
</blockquote>
<p>When considering a plane graph, we will observe a number of <strong>regions</strong> (also called<br><strong>faces</strong>), which are enclosed by the edges of the graph. Each region is enclosed by a cycle.</p>
<p>Euler’s formula: For a plane graph G with n vertices, m edges, and r regions, we<br>have that $n − m + r = 2$.</p>
<h2 id="Euler-tour"><a href="#Euler-tour" class="headerlink" title="Euler tour"></a>Euler tour</h2><p>A <strong>tour</strong> of a graph G is a <strong>(u, v)- closed walk</strong> that traverses <em>each edge</em> in G.</p>
<p>An <strong>Euler tour</strong> is a tour in which all edges are traversed <em>exactly once</em>.</p>
<p>A connected graph G has an Euler tour if and only if <strong><em>it has no vertices of odd degree</em></strong>.</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="Jiangfeng Du" />
          <p class="site-author-name" itemprop="name">Jiangfeng Du</p>
           
              <p class="site-description motion-element" itemprop="description">Notes for share and some plain essays</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">posts</span>
              </a>
            </div>
          

          

          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiangfeng Du</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.1"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
